{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Skin Lesion Analysis Towards Melanoma Detection\n",
    "\n",
    "---\n",
    "\n",
    "This notebook describes the creation of a convolutinal neural network to diagnose melanoma based on images.\n",
    "\n",
    "The data and objective are pulled from https://github.com/udacity/dermatologist-ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the images and show some samples to make sure the images have been downloaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "files = np.array(glob(\"./data/*/*/*\"))\n",
    "print('There are %d total images.' % len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline                               \n",
    "\n",
    "for i in range(5):\n",
    "    img = mpimg.imread(files[i])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a method to load the data. It takes the image transformations as an input parameter. This allows to conveniently evaluate the effect of different image transformations on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_data_loaders(data_dir, train_transform, test_transform, image_size, batch_size):\n",
    "    train_dir = os.path.join(data_dir, 'train/')\n",
    "    test_dir = os.path.join(data_dir, 'test/')\n",
    "    validation_dir = os.path.join(data_dir, 'valid/')\n",
    "    \n",
    "    # Resize() seems to truncate images sometimes. Set LOAD_TRUNCATED_IMAGES to avoid OSError \"image file is truncated\"\n",
    "    from PIL import ImageFile\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "    validation_data = datasets.ImageFolder(validation_dir, transform=test_transform)\n",
    "    \n",
    "    data = {\n",
    "        \"train\": train_data,\n",
    "        \"test\": test_data,\n",
    "        \"valid\": validation_data\n",
    "    }\n",
    "\n",
    "    # print out some data stats\n",
    "    print('Num training images: ', len(train_data))\n",
    "    print('Num test images: ', len(test_data))\n",
    "    print('Num validation images: ', len(validation_data))\n",
    "\n",
    "    # define dataloader parameters\n",
    "    num_workers = 0\n",
    "\n",
    "    # prepare data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                               num_workers=num_workers, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "                                              num_workers=num_workers, shuffle=True)\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, \n",
    "                                                   num_workers=num_workers, shuffle=True)\n",
    "\n",
    "    loaders = {\n",
    "        \"train\": train_loader,\n",
    "        \"test\": test_loader,\n",
    "        \"valid\": validation_loader\n",
    "    }\n",
    "    \n",
    "    # Visualize some images\n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    for idx in np.arange(batch_size):\n",
    "        ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "        plt.imshow(np.transpose(images[idx], (1, 2, 0)).astype(np.uint8))\n",
    "\n",
    "    return data, loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular technique to solve image classification problems is transfer learning. Transfer learning involves utilizing a pre-trained neural network and adapting it to a new dataset.\n",
    "\n",
    "Pytorch provides a number of state-of-the-art pre-trained CNN models. One of them is [Inception v3](https://arxiv.org/abs/1512.00567), trained on the 1000-class Imagenet dataset. It has been applied successfully on classification of skin lesion, for example by [A. Esteva in 2017](https://www.nature.com/articles/nature21056.epdf?author_access_token=8oxIcYWf5UNrNpHsUHd2StRgN0jAjWel9jnR3ZoTv0NXpMHRAJy8Qn10ys2O4tuPakXos4UhQAFZ750CsBNMMsISFHIKinKDMKjShCpHIlYPYUHhNzkn6pSnOCt0Ftf6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "def init_model(pretrained, freeze_weights, number_of_classes, use_cuda):\n",
    "    # instantiate a pre-trained Inception v3 model\n",
    "    model = models.inception_v3(pretrained=pretrained)\n",
    "\n",
    "    # freeze the pre-trained layers\n",
    "    if freeze_weights:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # remove the last fully connected layer of the pre-trained network \n",
    "    # and replace it by a new, randomly initialized layer\n",
    "    n_inputs = model.fc.in_features\n",
    "    model.fc = nn.Linear(n_inputs, number_of_classes)\n",
    "\n",
    "    n_inputs = model.AuxLogits.fc.in_features\n",
    "    model.AuxLogits.fc = nn.Linear(n_inputs, number_of_classes)\n",
    "\n",
    "    # move model to GPU if possible\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to see the model performance while training the model. \n",
    "\n",
    "`%matplotlib notebook` provides a interactive graph. It can be used to show the current training loss and validation loss, for example.\n",
    "\n",
    "I need a method to initialize such a graph and another method to update the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "def init_plot():\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(loc='center left') # the plot evolves to the right\n",
    "    return fig, ax\n",
    "\n",
    "def live_plot(fig, ax, data_dict, figsize=(7,5), title=''):\n",
    "    if ax.lines:\n",
    "        for line in ax.lines:\n",
    "            line.set_xdata(list(range(len(data_dict[line.get_label()]))))\n",
    "            line.set_ydata(data_dict[line.get_label()])\n",
    "        ax.set_xlim(0, len(data_dict[line.get_label()]))\n",
    "        \n",
    "    else:\n",
    "        for label,data in data_dict.items():\n",
    "            line, = ax.plot(data)\n",
    "            line.set_label(label)\n",
    "            ax.legend()\n",
    "        ax.set_ylim(0, max(data_dict.values())[0] + 0.5)\n",
    "    \n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "def train(n_epochs, loaders, model, criterion, optimizer, scheduler, use_cuda, save_path):\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    losses = collections.defaultdict(list)\n",
    "    fig,ax = init_plot()\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward\n",
    "            output = model(data)\n",
    "            \n",
    "            # The Inception model outputs two values: output from the last layer and the auxiliary logits.\n",
    "            # Ignore the second output.\n",
    "            if \"Inception3\" in str(type(model)):\n",
    "                output = output[0]\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            ## record the average training loss, using something like\n",
    "            train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Epoch: {} Batch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "                    epoch, \n",
    "                    batch_idx,\n",
    "                    train_loss\n",
    "                    ))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "        \n",
    "        # plot losses\n",
    "        if epoch > 1:\n",
    "            losses[\"train\"].append(train_loss.cpu())\n",
    "            losses[\"valid\"].append(valid_loss.cpu())\n",
    "            live_plot(fig, ax, losses)\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\t\\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tDuration: {:.6f} Min'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            (time.time() - epoch_start) / 60.\n",
    "            ))\n",
    "        \n",
    "        ## save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(\"validation loss has decreased --> save the model\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "    \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    # set model into evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, train and test a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning approaches\n",
    "\n",
    "There are different approaches to do transfer learning. If the the size of the new dataset is small, layers of the pre-trained model can be sliced off, the weights of the remaining layers frozen and a new fully connected layer can be added. This is also referred to as feature extraction. If the the size of the new dataset is large, the pre-trained model can be fine-tuned or re-trained - depending on the similarity of the new dataset to the original training data.\n",
    "\n",
    "I want to compare the different approaches. I expect fine-tuning to preduce the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=299 # inception_v3 expects tensors with a size of N x 3 x 299 x 299\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                      transforms.RandomVerticalFlip(p=0.5),\n",
    "                                      transforms.Resize((image_size, image_size)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                           std=[0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((image_size, image_size)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "data, loaders = get_data_loaders(\"data/\", train_transform, test_transform, image_size, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cuda if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# model parameters\n",
    "number_of_classes = len(data[\"train\"].classes) # 3 classes: melanoma, nevus and seborrheic_keratosis\n",
    "\n",
    "# optimization algorithm parameters set according to Esteva et al, Dermatologist-level classification of \n",
    "# skin cancer with deep neural networks. Nature. 2017;542:115\n",
    "learning_rate = 0.001\n",
    "epsilon = 0.1\n",
    "weight_decay = 0.9\n",
    "momentum = 0.9\n",
    "scheduler_step_size = 30\n",
    "decay_factor = 1./16.\n",
    "\n",
    "# training parameters\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports required later on\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a pre-trained Inception v3 model and freeze the pre-trained weights\n",
    "model = init_model(pretrained=True, freeze_weights=True, number_of_classes=number_of_classes, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.fc.parameters(), lr=learning_rate, eps=epsilon, weight_decay=weight_decay, \n",
    "                          momentum=momentum)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=decay_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(n_epochs, loaders, model, criterion, optimizer, scheduler, use_cuda, \n",
    "              'model_dermatologist_feature_extraction.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that got the best validation accuracy\n",
    "model.load_state_dict(torch.load('model_dermatologist_fine-tuned.pt'))\n",
    "\n",
    "test(loaders, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Retrain Inception v3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a pre-trained Inception v3 model which is initialized with the random weights\n",
    "model = init_model(pretrained=False, freeze_weights=False, number_of_classes=number_of_classes, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.fc.parameters(), lr=learning_rate, eps=epsilon, weight_decay=weight_decay, \n",
    "                          momentum=momentum)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=decay_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(n_epochs, loaders, model, criterion, optimizer, scheduler,\n",
    "              use_cuda, 'model_dermatologist_re-trained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that got the best validation accuracy\n",
    "model.load_state_dict(torch.load('model_dermatologist_re-trained.pt'))\n",
    "\n",
    "test(loaders, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: Fine-Tune Inception v3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a pre-trained Inception v3 model which is initialized with the pre-trained weights\n",
    "model = init_model(pretrained=True, freeze_weights=False, number_of_classes=number_of_classes, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.fc.parameters(), lr=learning_rate, eps=epsilon, weight_decay=weight_decay, \n",
    "                          momentum=momentum)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=decay_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(n_epochs, loaders, model, criterion, optimizer, scheduler, use_cuda, \n",
    "              'model_dermatologist_fine-tuned.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that got the best validation accuracy\n",
    "model.load_state_dict(torch.load('model_dermatologist_fine-tuned.pt'))\n",
    "\n",
    "test(loaders, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from PIL import Image\n",
    "\n",
    "loader = transforms.Compose([transforms.Resize((299, 299)), transforms.ToTensor()])\n",
    "\n",
    "def image_loader(file_name):\n",
    "    image = Image.open(file_name)\n",
    "    image = loader(image).float()\n",
    "    #image = Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def csv_dump(model, use_cuda):\n",
    "    model.eval()\n",
    "    \n",
    "    with open(\"predictions.csv\", mode='w') as output_file:\n",
    "        writer = csv.writer(output_file, delimiter=',', lineterminator='\\n', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(['Id', 'task_1', 'task_2'])\n",
    "        \n",
    "        files = np.array(glob(\"./data/test/*/*\"))\n",
    "        for file in files:\n",
    "            data = image_loader(file)\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # task 1: the model's predicted probability that the image (at the path in Id) depicts melanoma\n",
    "            p_melanoma = np.squeeze(output.data[:,0:1]).cpu().numpy()\n",
    "            # task 2: the model's predicted probability that the image (at the path in Id) depicts seborrheic keratosis\n",
    "            p_seborrheic_keratosis = np.squeeze(output.data[:,2:3]).cpu().numpy()\n",
    "            \n",
    "            writer.writerow([file.replace('\\\\', '/'), p_melanoma, p_seborrheic_keratosis])\n",
    "\n",
    "csv_dump(model, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i get_results.py predictions.csv"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
